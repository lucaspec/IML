{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "X_pretrain_import = np.genfromtxt('pretrain_features.csv', delimiter=',')\n",
    "y_pretrain_import = np.genfromtxt('pretrain_labels.csv', delimiter=',')\n",
    "X_train_import = np.genfromtxt('train_features.csv', delimiter=',')\n",
    "y_train_import = np.genfromtxt('train_labels.csv', delimiter=',')\n",
    "\n",
    "X_predict_import = np.genfromtxt('test_features.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pretrain = X_pretrain_import[1:, 2:]\n",
    "y_pretrain = y_pretrain_import[1:, 1:]\n",
    "X_train = X_train_import[1:, 2:]\n",
    "y_train = y_train_import[1:, 1:]\n",
    "\n",
    "X_predict = X_predict_import[1:, 2:]\n",
    "\n",
    "\n",
    "X_predict_names = X_predict_import[1:,0]\n",
    "print(X_predict_names.shape)\n",
    "\n",
    "print(X_pretrain.shape)\n",
    "print(y_pretrain.shape)\n",
    "print()\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print()\n",
    "print(X_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split for pretraining data (lumo)\n",
    "X_pretrain_test, X_pretrain_train, y_pretrain_test, y_pretrain_train =  train_test_split(X_pretrain, y_pretrain, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split for training data (homo-lumo)\n",
    "X_test, X_train, y_test, y_train =  train_test_split(X_train, y_train, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "print(X_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "class Dataset(Dataset):\n",
    "      def __init__(self, labels, data):\n",
    "            self.labels = labels\n",
    "            self.data = data\n",
    "\n",
    "      def __len__(self):\n",
    "            return self.labels.shape[0]\n",
    "\n",
    "      def __getitem__(self, index):\n",
    "            # Load data and get label\n",
    "            X = self.data[index,:]\n",
    "            y = self.labels[index,:]\n",
    "\n",
    "            return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generators\n",
    "params_pretrain = {'batch_size': 500,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "\n",
    "params_validate = {'batch_size': 1000,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "\n",
    "params_train = {'batch_size': 100,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "\n",
    "\n",
    "training_set = Dataset(y_pretrain_train, X_pretrain_train)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **params_pretrain)\n",
    "\n",
    "validation_set = Dataset(y_pretrain_test, X_pretrain_test)\n",
    "validation_generator = torch.utils.data.DataLoader(validation_set, **params_validate)\n",
    "\n",
    "final_set = Dataset(y_train, X_train)\n",
    "final_generator = torch.utils.data.DataLoader(final_set, **params_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple neural network\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(1000, 500)\n",
    "        self.fc2 = nn.Linear(500, 250)\n",
    "        self.fc3 = nn.Linear(250, 100)\n",
    "        self.fc4 = nn.Linear(100, 50)\n",
    "        self.fc5 = nn.Linear(50, 1)\n",
    "        #self.fc6 = nn.Linear(25, 10)\n",
    "        #self.fc7 = nn.Linear(10, 1)\n",
    "        self.activation_fn = torch.nn.ReLU()\n",
    "        #self.activation_fn = torch.nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation_fn(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.activation_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.activation_fn(x)\n",
    "        x = self.fc5(x)\n",
    "        #x = self.activation_fn(x)\n",
    "        #x = self.fc6(x)\n",
    "        #x = self.activation_fn(x)\n",
    "        #x = self.fc7(x)\n",
    "        return x\n",
    "\n",
    "# optimizer Adam\n",
    "net = Net()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0005, weight_decay=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss\n",
    "criterion = nn.MSELoss()\n",
    "net.fc4.requires_grad = False\n",
    "net.fc5.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretraining loop\n",
    "max_epochs = 100\n",
    "losses = list()\n",
    "validation = list()\n",
    "train_losses = list()\n",
    "for epoch in tqdm(range(max_epochs)):\n",
    "    for local_batch, local_labels in training_generator:\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        output = net(local_batch.float())\n",
    "        loss = criterion(output.float(), local_labels.float())\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "\n",
    "        validationset = next(iter(validation_generator))\n",
    "        validation_features = validationset[0]\n",
    "        validation_labels = validationset[1]\n",
    "        validation_output = net(validation_features.float())\n",
    "        validation_loss = criterion(validation_output.float(), validation_labels.float())\n",
    "        validation.append(validation_loss.item())\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(loss)\n",
    "print(validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot pretain loss\n",
    "plt.plot(losses, color='blue')\n",
    "plt.plot(validation, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze weights of first layers\n",
    "net.fc1.requires_grad = False\n",
    "net.fc2.requires_grad = False\n",
    "net.fc3.requires_grad = False\n",
    "net.fc4.requires_grad = True\n",
    "net.fc5.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "train_losses = list()\n",
    "test_losses = list()\n",
    "max_iter = 10000\n",
    "test_features = torch.from_numpy(X_test)\n",
    "test_labels = torch.from_numpy(y_test)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.002, momentum=0, dampening=0, weight_decay=0, nesterov=False, maximize=False)\n",
    "\n",
    "for iter in tqdm(range(max_iter)):\n",
    "    for local_batch, local_labels in final_generator:\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        output = net(local_batch.float())\n",
    "        loss = criterion(output.float(), local_labels.float())\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        test_output = net(test_features.float())\n",
    "        test_loss = criterion(test_output.float(), test_labels.float())\n",
    "        test_losses.append(test_loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if math.sqrt(test_loss) < 0.19:\n",
    "        break\n",
    "\n",
    "print('MSE of test set:', test_loss)\n",
    "print('RMSE of test set:', math.sqrt(test_loss))\n",
    "\n",
    "print('MSE of training set:', loss)\n",
    "print('RMSE of training set:', math.sqrt(loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train loss\n",
    "plt.plot(train_losses, color='blue')\n",
    "plt.plot(test_losses, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "test_features = torch.from_numpy(X_predict)\n",
    "test_output = net(test_features.float())\n",
    "\n",
    "output = test_output.detach().numpy()\n",
    "\n",
    "out = pd.DataFrame(output, index = X_predict_names.astype(int).astype(str), columns = ['y'])\n",
    "\n",
    "print(out)\n",
    "out.to_csv('firstsub.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d62c07d2536fe2bab6fa562096e193a4fadc23349d25978ae013d3193ab7aab6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
